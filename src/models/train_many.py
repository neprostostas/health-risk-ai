"""
–ú–æ–¥—É–ª—å –Ω–∞–≤—á–∞–Ω–Ω—è –º–Ω–æ–∂–∏–Ω–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ä–∏–∑–∏–∫—ñ–≤ –∑–¥–æ—Ä–æ–≤'—è.
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Tuple

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.calibration import calibration_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    brier_score_loss,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
)
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC

# –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏ –¥–ª—è XGBoost —Ç–∞ LightGBM
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è XGBoost –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –ú–æ–¥–µ–ª—å XGBoost –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")

try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except (ImportError, OSError, Exception):
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è LightGBM –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π. –ú–æ–¥–µ–ª—å LightGBM –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤
PROJECT_ROOT = Path(__file__).resolve().parents[2]
DATA_PATH = PROJECT_ROOT / "datasets/processed/health_dataset.csv"
MODELS_DIR = PROJECT_ROOT / "artifacts/models"

# –¶—ñ–ª—å–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ
TARGETS = ["diabetes_present", "obesity_present"]

# –ë–∞–∑–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ (LBXGLU –º–æ–∂–µ –±—É—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—è –≤ –¥–∞—Ç–∞—Å–µ—Ç—ñ)
BASE_FEATURES = ["RIDAGEYR", "RIAGENDR", "BMXBMI", "BPXSY1", "BPXDI1", "LBXTC"]
# LBXGLU –¥–æ–¥–∞–º–æ, —è–∫—â–æ –≤–æ–Ω–∞ —ñ—Å–Ω—É—î –≤ –¥–∞—Ç–∞—Å–µ—Ç—ñ

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
TEST_SIZE = 0.2
RANDOM_STATE = 42

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
plt.rcParams["figure.figsize"] = (10, 8)
plt.rcParams["font.size"] = 10


def load_and_prepare_data(
    data_path: Path, target: str, features: List[str]
) -> Tuple[pd.DataFrame, pd.Series, List[str]]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ —Ç–∞ –≥–æ—Ç—É—î —ó—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è.
    
    Args:
        data_path: –®–ª—è—Ö –¥–æ CSV —Ñ–∞–π–ª—É –∑ –¥–∞–Ω–∏–º–∏
        target: –ù–∞–∑–≤–∞ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
        features: –°–ø–∏—Å–æ–∫ –æ–∑–Ω–∞–∫ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (X, y, available_features)
    """
    print(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ {data_path}...")
    df = pd.read_csv(data_path, encoding="utf-8")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
    available_features = [f for f in features if f in df.columns]
    missing_features = [f for f in features if f not in df.columns]
    
    if missing_features:
        print(f"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –æ–∑–Ω–∞–∫–∏: {missing_features}")
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è LBXGLU, —è–∫—â–æ –≤–æ–Ω–∞ —î –≤ –¥–∞—Ç–∞—Å–µ—Ç—ñ
    if "LBXGLU" in df.columns and "LBXGLU" not in available_features:
        available_features.append("LBXGLU")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
    if target not in df.columns:
        raise ValueError(f"–¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞ '{target}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç—ñ")
    
    # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –≤ –æ–±—Ä–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö —Ç–∞ —Ü—ñ–ª—å–æ–≤—ñ–π –∑–º—ñ–Ω–Ω—ñ–π
    required_cols = available_features + [target]
    df_clean = df[required_cols].dropna()
    
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df_clean)} —Ä—è–¥–∫—ñ–≤ –∑ {len(available_features)} –æ–∑–Ω–∞–∫–∞–º–∏")
    
    X = df_clean[available_features]
    y = df_clean[target]
    
    return X, y, available_features


def create_preprocessing_pipeline(numeric_features: List[str], categorical_features: List[str]) -> ColumnTransformer:
    """
    –°—Ç–≤–æ—Ä—é—î –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö.
    
    Args:
        numeric_features: –°–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
        categorical_features: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
    
    Returns:
        ColumnTransformer –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏
    """
    # –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫: —ñ–º–ø—É—Ç–∞—Ü—ñ—è –º–µ–¥—ñ–∞–Ω–æ—é -> —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è
    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )
    
    # –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫: —ñ–º–ø—É—Ç–∞—Ü—ñ—è –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º -> one-hot encoding
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False, drop="first")),
        ]
    )
    
    # –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ],
        remainder="drop",
    )
    
    return preprocessor


def get_models() -> Dict[str, object]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è.
    
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –Ω–∞–∑–≤–∞–º–∏ –º–æ–¥–µ–ª–µ–π —Ç–∞ —ó—Ö –µ–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏
    """
    models = {
        "LogisticRegression": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),
        "RandomForest": RandomForestClassifier(
            n_estimators=300, max_depth=None, n_jobs=-1, random_state=RANDOM_STATE
        ),
        "SVC": SVC(
            kernel="rbf", probability=True, C=2.0, gamma="scale", random_state=RANDOM_STATE
        ),
        "KNN": KNeighborsClassifier(n_neighbors=15),
        "MLP": MLPClassifier(
            hidden_layer_sizes=(64, 32),
            activation="relu",
            max_iter=300,
            random_state=RANDOM_STATE,
        ),
    }
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è XGBoost, —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π
    if XGBOOST_AVAILABLE:
        models["XGBoost"] = XGBClassifier(
            n_estimators=400,
            learning_rate=0.05,
            max_depth=4,
            subsample=0.9,
            colsample_bytree=0.9,
            eval_metric="logloss",
            random_state=RANDOM_STATE,
            n_jobs=-1,
        )
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è LightGBM, —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π
    if LIGHTGBM_AVAILABLE:
        models["LightGBM"] = LGBMClassifier(
            n_estimators=400,
            learning_rate=0.05,
            max_depth=-1,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbose=-1,
        )
    
    return models


def get_predict_proba(model, X: np.ndarray) -> np.ndarray:
    """
    –û—Ç—Ä–∏–º—É—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –≤—ñ–¥ –º–æ–¥–µ–ª—ñ.
    
    Args:
        model: –ù–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å
        X: –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
    
    Returns:
        –ú–∞—Å–∏–≤ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å
    """
    if hasattr(model, "predict_proba"):
        return model.predict_proba(X)[:, 1]
    elif hasattr(model, "decision_function"):
        # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è decision_function –Ω–∞ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ —á–µ—Ä–µ–∑ —Å–∏–≥–º–æ—ó–¥—É
        decision = model.decision_function(X)
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É [0, 1]
        proba = 1 / (1 + np.exp(-decision))
        return proba
    else:
        raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î predict_proba –∞–±–æ decision_function")


def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray) -> Dict[str, float]:
    """
    –û–±—á–∏—Å–ª—é—î –º–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ.
    
    Args:
        y_true: –°–ø—Ä–∞–≤–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        y_pred: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (–±—ñ–Ω–∞—Ä–Ω—ñ)
        y_proba: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
    
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    metrics = {
        "roc_auc": roc_auc_score(y_true, y_proba),
        "avg_precision": average_precision_score(y_true, y_proba),
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall": recall_score(y_true, y_pred, zero_division=0),
        "f1": f1_score(y_true, y_pred, zero_division=0),
        "brier": brier_score_loss(y_true, y_proba),
    }
    
    return metrics


def plot_roc_curve(y_true: np.ndarray, y_proba: np.ndarray, save_path: Path) -> None:
    """
    –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è ROC-–∫—Ä–∏–≤–æ—ó.
    
    Args:
        y_true: –°–ø—Ä–∞–≤–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        y_proba: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
        save_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
    """
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = roc_auc_score(y_true, y_proba)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC –∫—Ä–∏–≤–∞ (AUC = {roc_auc:.3f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="–í–∏–ø–∞–¥–∫–æ–≤–∞ –º–æ–¥–µ–ª—å")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("–ß–∞—Å—Ç–∫–∞ —Ö–∏–±–Ω–æ–ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö (False Positive Rate)")
    plt.ylabel("–ß–∞—Å—Ç–∫–∞ —ñ—Å—Ç–∏–Ω–Ω–æ–ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö (True Positive Rate)")
    plt.title("ROC-–∫—Ä–∏–≤–∞")
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()


def plot_pr_curve(y_true: np.ndarray, y_proba: np.ndarray, save_path: Path) -> None:
    """
    –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è Precision-Recall –∫—Ä–∏–≤–æ—ó.
    
    Args:
        y_true: –°–ø—Ä–∞–≤–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        y_proba: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
        save_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
    """
    precision, recall, _ = precision_recall_curve(y_true, y_proba)
    avg_precision = average_precision_score(y_true, y_proba)
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color="blue", lw=2, label=f"PR –∫—Ä–∏–≤–∞ (AP = {avg_precision:.3f})")
    plt.xlabel("–ü–æ–≤–Ω–æ—Ç–∞ (Recall)")
    plt.ylabel("–¢–æ—á–Ω—ñ—Å—Ç—å (Precision)")
    plt.title("Precision-Recall –∫—Ä–∏–≤–∞")
    plt.legend(loc="lower left")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()


def plot_calibration_curve(y_true: np.ndarray, y_proba: np.ndarray, save_path: Path) -> None:
    """
    –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫—Ä–∏–≤–æ—ó –∫–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è.
    
    Args:
        y_true: –°–ø—Ä–∞–≤–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        y_proba: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
        save_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
    """
    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_proba, n_bins=10)
    
    plt.figure(figsize=(8, 6))
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="–ú–æ–¥–µ–ª—å")
    plt.plot([0, 1], [0, 1], "k--", label="–Ü–¥–µ–∞–ª—å–Ω–∞ –∫–∞–ª—ñ–±—Ä–æ–≤–∫–∞")
    plt.xlabel("–°–µ—Ä–µ–¥–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å")
    plt.ylabel("–ß–∞—Å—Ç–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö")
    plt.title("–ö—Ä–∏–≤–∞ –∫–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è")
    plt.legend(loc="upper left")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()


def plot_feature_importance(importances: Dict[str, float], save_path: Path) -> None:
    """
    –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫.
    
    Args:
        importances: –°–ª–æ–≤–Ω–∏–∫ –∑ –Ω–∞–∑–≤–∞–º–∏ –æ–∑–Ω–∞–∫ —Ç–∞ —ó—Ö –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
        save_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
    """
    features = list(importances.keys())
    values = list(importances.values())
    
    # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
    sorted_indices = np.argsort(values)[::-1]
    features = [features[i] for i in sorted_indices]
    values = [values[i] for i in sorted_indices]
    
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(features)), values)
    plt.yticks(range(len(features)), features)
    plt.xlabel("–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫–∏")
    plt.title("–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ (Permutation Importance)")
    plt.gca().invert_yaxis()
    plt.grid(True, alpha=0.3, axis="x")
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()


def train_model_for_target(
    X: pd.DataFrame,
    y: pd.Series,
    target: str,
    models: Dict[str, object],
    available_features: List[str],
) -> Dict[str, Dict]:
    """
    –ù–∞–≤—á–∞—î –≤—Å—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –æ–¥–Ω—ñ—î—ó —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó.
    
    Args:
        X: –û–∑–Ω–∞–∫–∏
        y: –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞
        target: –ù–∞–∑–≤–∞ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
        models: –°–ª–æ–≤–Ω–∏–∫ –∑ –º–æ–¥–µ–ª—è–º–∏
        available_features: –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –æ–∑–Ω–∞–∫
    
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    """
    print(f"\n{'='*80}")
    print(f"–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó: {target}")
    print(f"{'='*80}")
    
    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—É —Ç–∞ —Ç–µ—Å—Ç–æ–≤—É –≤–∏–±—ñ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
    )
    
    print(f"–†–æ–∑–º—ñ—Ä —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏: {len(X_train)}")
    print(f"–†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ—ó –≤–∏–±—ñ—Ä–∫–∏: {len(X_test)}")
    
    # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—ñ–≤ –æ–∑–Ω–∞–∫
    numeric_features = [f for f in available_features if f != "RIAGENDR"]
    categorical_features = [f for f in available_features if f == "RIAGENDR"]
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞–π–ø–ª–∞–π–Ω—É –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏
    preprocessor = create_preprocessing_pipeline(numeric_features, categorical_features)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    target_dir = MODELS_DIR / target
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    results = {}
    leaderboard_data = []
    
    # –ù–∞–≤—á–∞–Ω–Ω—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
    for model_name, model in models.items():
        print(f"\nüîπ –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {model_name}")
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É
            pipeline = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
            
            # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            pipeline.fit(X_train, y_train)
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ
            y_pred = pipeline.predict(X_test)
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ pipeline
            try:
                y_proba = pipeline.predict_proba(X_test)[:, 1]
            except Exception:
                # Fallback –¥–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ predict_proba
                X_test_transformed = pipeline.named_steps["preprocessor"].transform(X_test)
                y_proba = get_predict_proba(pipeline.named_steps["model"], X_test_transformed)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è permutation importance
            X_test_transformed = pipeline.named_steps["preprocessor"].transform(X_test)
            
            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
            metrics = compute_metrics(y_test, y_pred, y_proba)
            
            print(f"  ROC-AUC: {metrics['roc_auc']:.4f}")
            print(f"  Average Precision: {metrics['avg_precision']:.4f}")
            print(f"  F1: {metrics['f1']:.4f}")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
            model_dir = target_dir / model_name
            model_dir.mkdir(parents=True, exist_ok=True)
            
            with open(model_dir / "metrics.json", "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            # –ü–æ–±—É–¥–æ–≤–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
            plot_roc_curve(y_test, y_proba, model_dir / "roc.png")
            plot_pr_curve(y_test, y_proba, model_dir / "pr.png")
            plot_calibration_curve(y_test, y_proba, model_dir / "calibration.png")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            joblib.dump(pipeline, model_dir / "model.joblib")
            
            # –î–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ –ª—ñ–¥–µ—Ä–±–æ—Ä–¥—É
            leaderboard_data.append(
                {
                    "model": model_name,
                    "roc_auc": metrics["roc_auc"],
                    "avg_precision": metrics["avg_precision"],
                    "accuracy": metrics["accuracy"],
                    "precision": metrics["precision"],
                    "recall": metrics["recall"],
                    "f1": metrics["f1"],
                    "brier": metrics["brier"],
                }
            )
            
            results[model_name] = {
                "pipeline": pipeline,
                "metrics": metrics,
                "X_test": X_test,
                "y_test": y_test,
                "X_test_transformed": X_test_transformed,
            }
            
            print(f"  ‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {model_dir}")
            
        except Exception as e:
            print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ {model_name}: {str(e)}")
            continue
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª—ñ–¥–µ—Ä–±–æ—Ä–¥—É
    if leaderboard_data:
        leaderboard = pd.DataFrame(leaderboard_data)
        leaderboard = leaderboard.sort_values(
            by=["roc_auc", "avg_precision"], ascending=[False, False]
        )
        leaderboard.to_csv(target_dir / "leaderboard.csv", index=False)
        
        print(f"\nüìä –õ—ñ–¥–µ—Ä–±–æ—Ä–¥ –¥–ª—è {target}:")
        print(leaderboard[["model", "roc_auc", "avg_precision", "f1"]].head(3).to_string(index=False))
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —á–µ–º–ø—ñ–æ–Ω–∞
        champion_row = leaderboard.iloc[0]
        champion_name = champion_row["model"]
        
        # –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —á–µ–º–ø—ñ–æ–Ω–∞
        champion_metrics = None
        for item in leaderboard_data:
            if item["model"] == champion_name:
                champion_metrics = item
                break
        
        if champion_metrics is None:
            champion_metrics = champion_row.to_dict()
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —á–µ–º–ø—ñ–æ–Ω–∞
        champion_metadata = {
            "model_name": champion_name,
            "path": str(target_dir / champion_name / "model.joblib"),
            "metrics": champion_metrics,
        }
        
        with open(target_dir / "champion.json", "w", encoding="utf-8") as f:
            json.dump(champion_metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nüèÜ –ß–µ–º–ø—ñ–æ–Ω –¥–ª—è {target}: {champion_name}")
        print(f"   ROC-AUC: {champion_metrics['roc_auc']:.4f}")
        print(f"   Average Precision: {champion_metrics['avg_precision']:.4f}")
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ –¥–ª—è —á–µ–º–ø—ñ–æ–Ω–∞
        if champion_name in results:
            print(f"\nüîç –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ –¥–ª—è —á–µ–º–ø—ñ–æ–Ω–∞...")
            try:
                champion_pipeline = results[champion_name]["pipeline"]
                X_test_champ = results[champion_name]["X_test"]
                y_test_champ = results[champion_name]["y_test"]
                X_test_transformed_champ = results[champion_name]["X_test_transformed"]
                
                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–∑–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫
                preprocessor = champion_pipeline.named_steps["preprocessor"]
                
                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–∑–≤ –æ–∑–Ω–∞–∫ –ø—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
                try:
                    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞–∑–≤–∏ –æ–∑–Ω–∞–∫ —á–µ—Ä–µ–∑ get_feature_names_out
                    if hasattr(preprocessor, "get_feature_names_out"):
                        feature_names = list(preprocessor.get_feature_names_out(available_features))
                    else:
                        # Fallback: –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –Ω–∞–∑–≤ –æ–∑–Ω–∞–∫
                        feature_names = numeric_features.copy()
                        if categorical_features:
                            # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ –¥–æ–¥–∞—î–º–æ –∑–∞–∫–æ–¥–æ–≤–∞–Ω—ñ –Ω–∞–∑–≤–∏
                            # OneHotEncoder –∑ drop='first' —Å—Ç–≤–æ—Ä—é—î n-1 –∫–æ–ª–æ–Ω–æ–∫
                            for i in range(len(categorical_features)):
                                # –ó–∞–∑–≤–∏—á–∞–π —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è RIAGENDR (—è–∫—â–æ drop='first')
                                feature_names.append(f"RIAGENDR_encoded")
                except Exception as e:
                    # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–∞–∑–≤–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ
                    feature_names = numeric_features.copy()
                    if categorical_features:
                        feature_names.append("RIAGENDR_encoded")
                    print(f"  ‚ö†Ô∏è –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ fallback –¥–ª—è –Ω–∞–∑–≤ –æ–∑–Ω–∞–∫: {str(e)}")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
                n_features_transformed = X_test_transformed_champ.shape[1]
                if len(feature_names) != n_features_transformed:
                    # –Ø–∫—â–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ —Å–ø—ñ–≤–ø–∞–¥–∞—î, —Å—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω—ñ –Ω–∞–∑–≤–∏
                    feature_names = [f"feature_{i}" for i in range(n_features_transformed)]
                
                # Permutation importance
                model = champion_pipeline.named_steps["model"]
                perm_importance = permutation_importance(
                    model, X_test_transformed_champ, y_test_champ, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1
                )
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
                importances_dict = {
                    feature_names[i]: float(perm_importance.importances_mean[i])
                    for i in range(min(len(feature_names), len(perm_importance.importances_mean)))
                }
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
                with open(target_dir / "champion_importance.json", "w", encoding="utf-8") as f:
                    json.dump(importances_dict, f, indent=2, ensure_ascii=False)
                
                # –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
                plot_feature_importance(importances_dict, target_dir / "champion_importance.png")
                
                print(f"  ‚úÖ –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫: {str(e)}")
                import traceback
                traceback.print_exc()
    
    return results


def main() -> None:
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É –Ω–∞–≤—á–∞–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π."""
    print("=" * 80)
    print("–ó–ê–ü–£–°–ö –ù–ê–í–ß–ê–ù–ù–Ø –ú–û–î–ï–õ–ï–ô –ú–ê–®–ò–ù–ù–û–ì–û –ù–ê–í–ß–ê–ù–ù–Ø")
    print("=" * 80)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è –º–æ–¥–µ–ª–µ–π
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    
    # –ù–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
    all_results = {}
    models = get_models()
    
    for target in TARGETS:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        X, y, available_features = load_and_prepare_data(DATA_PATH, target, BASE_FEATURES)
        
        # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
        results = train_model_for_target(X, y, target, models, available_features)
        all_results[target] = results
    
    # –§—ñ–Ω–∞–ª—å–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
    print("\n" + "=" * 80)
    print("‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—ñ–¥–µ—Ä–±–æ—Ä–¥–∏ —Ç–∞ —á–µ–º–ø—ñ–æ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É artifacts/models/")
    print("=" * 80)
    
    # –í–∏–≤–µ–¥–µ–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–æ –ª—ñ–¥–µ—Ä–±–æ—Ä–¥—ñ–≤ —Ç–∞ —á–µ–º–ø—ñ–æ–Ω—ñ–≤
    for target in TARGETS:
        target_dir = MODELS_DIR / target
        leaderboard_path = target_dir / "leaderboard.csv"
        champion_path = target_dir / "champion.json"
        
        if leaderboard_path.exists():
            print(f"\nüìä {target}:")
            print(f"   –õ—ñ–¥–µ—Ä–±–æ—Ä–¥: {leaderboard_path}")
            if champion_path.exists():
                print(f"   –ß–µ–º–ø—ñ–æ–Ω: {champion_path}")


if __name__ == "__main__":
    main()

