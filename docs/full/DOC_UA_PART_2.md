# Частина 2: ML-моделі та API

## 6. ML-Моделі (Machine Learning)

### 6.1. Мета ML у проєкті

Машинне навчання в проєкті HealthRisk.AI виконує роль інтелектуального ядра системи, яке трансформує медичні параметри користувачів в об'єктивні оцінки ризиків здоров'я. Основна мета ML-компонента полягає в тому, щоб надати точні та надійні прогнози ризиків розвитку захворювань на основі обмеженого набору медичних показників, які користувач може легко надати через веб-інтерфейс.

**Які ризики прогнозуються** — система навчає та використовує моделі для прогнозування двох ключових ризиків здоров'я: ожиріння та діабету. Ожиріння визначається як наявність індексу маси тіла (ІМТ) рівного або вище 30, що відповідає критеріям Всесвітньої організації охорони здоров'я. Діабет визначається як діагностований діабет згідно з опитувальником NHANES. Обидва ризики є бінарними змінними (наявність або відсутність), що робить задачу класифікацією з двома класами.

**Чому ці показники** — вибір саме цих двох ризиків обумовлений кількома факторами. По-перше, обидва захворювання є широко поширеними та мають значний вплив на здоров'я населення, що робить їх актуальними для оцінки. По-друге, обидва ризики мають чіткі медичні критерії визначення, що дозволяє точно формувати цільові змінні для навчання моделей. По-третє, обидва ризики пов'язані з факторами, які можна легко виміряти або отримати від користувача (вік, стать, ІМТ, артеріальний тиск, лабораторні показники), що робить систему практичною для використання. По-четверте, обидва ризики мають достатню кількість позитивних випадків у датасеті NHANES для навчання надійних моделей (24% ожиріння, 7.5% діабету).

**Які змінні впливають** — моделі використовують наступний набір ознак для прогнозування ризиків: вік особи (RIDAGEYR) — важлива ознака, оскільки старший вік асоціюється з вищими ризиками обох захворювань; стать (RIAGENDR) — використовується для врахування статевих відмінностей у розподілі ризиків; індекс маси тіла (BMXBMI) — ключова ознака для прогнозування ожиріння та пов'язаних з ним ризиків; систолічний артеріальний тиск (BPXSY1) та діастолічний артеріальний тиск (BPXDI1) — важливі показники, які корелюють з обома цільовими змінними; загальний холестерин (LBXTC) — показник, який демонструє кореляцію з ІМТ та загальним станом здоров'я; рівень глюкози в крові (LBXGLU) — критична ознака для прогнозування діабету, додається до моделей, якщо доступна в датасеті. Ці ознаки були відібрані на основі результатів EDA, які показали їх значущість та кореляцію з цільовими змінними.

### 6.2. Підготовка даних для моделей

Підготовка даних для навчання моделей є критично важливим етапом, який забезпечує якість та надійність прогнозів. Процес підготовки включає кілька ключових кроків, кожен з яких виконує специфічну роль у забезпеченні готовності даних для машинного навчання.

**Normalization** — нормалізація даних виконується через StandardScaler, який перетворює числові ознаки до стандартного розподілу з середнім значенням 0 та стандартним відхиленням 1. Це критично важливо для моделей, які чутливі до масштабу даних, таких як логістична регресія, SVM та нейромережі. Нормалізація забезпечує, що всі ознаки мають однаковий вплив на модель незалежно від їхніх оригінальних діапазонів значень. Наприклад, вік (діапазон 0-100) та холестерин (діапазон 100-300) після нормалізації мають однаковий масштаб, що дозволяє моделі правильно оцінити їхню важливість. Нормалізація виконується на тренувальній вибірці, після чого параметри нормалізації (середнє та стандартне відхилення) зберігаються та застосовуються до тестової вибірки та нових даних під час інференсу.

**Train-test split** — датасет розділяється на тренувальну та тестову вибірки у співвідношенні 80/20, що забезпечує достатню кількість даних для навчання та надійну оцінку продуктивності моделей. Розділення виконується зі стратифікацією за цільовою змінною, що означає, що пропорції позитивних та негативних класів зберігаються в обох вибірках. Це критично важливо для незбалансованих датасетів, де позитивні випадки становлять меншість (7.5% для діабету, 24% для ожиріння). Стратифікація забезпечує, що модель навчається та тестується на репрезентативних вибірках, що покращує надійність оцінки метрик. Використовується `random_state=42` для забезпечення відтворюваності результатів, що дозволяє порівнювати різні моделі на однакових вибірках.

**Вибір ознак** — відбір ознак виконується на основі результатів EDA, які показали значущість та кореляцію з цільовими змінними. Всі моделі використовують однаковий набір ознак для забезпечення порівнянності результатів. Ознаки відібрані на основі кількох критеріїв: наявність значущого зв'язку з цільовими змінними (виявлено через кореляційний аналіз та візуалізації), доступність для користувачів (всі ознаки можуть бути легко виміряні або отримані), відсутність надмірної кореляції між ознаками (для уникнення мультиколінеарності), достатня кількість доступних даних (ознаки з надто багатьма пропусками виключаються або обробляються спеціально).

**Чому саме такі фактори** — вибір саме цих факторів обґрунтований медичною наукою та результатами EDA. Вік є універсальним фактором ризику для більшості захворювань, включаючи діабет та ожиріння, що підтверджено численними дослідженнями. Стать враховує статеві відмінності у розподілі ризиків, які спостерігаються в реальних даних. ІМТ є прямим показником ожиріння та тісно пов'язаний з метаболічними ризиками, включаючи діабет. Артеріальний тиск є маркером серцево-судинного здоров'я та корелює з обома цільовими змінними. Холестерин є показником метаболічного стану та пов'язаний з ризиками діабету та серцево-судинних захворювань. Глюкоза є прямим маркером діабету та критично важлива для прогнозування цього ризику. Результати EDA підтвердили значущість цих факторів через кореляційний аналіз та візуалізації, що обґрунтувало їх включення в моделі.

**Проблеми дисбалансу** — датасет має дисбаланс класів, де позитивні випадки становлять меншість (7.5% для діабету, 24% для ожиріння). Це створює проблему для моделей, які можуть навчитися завжди передбачати більшість клас, що призводить до високої точності, але низької чутливості до позитивних випадків. Для вирішення цієї проблеми використовуються кілька підходів: стратифікація при розділенні на тренувальну та тестову вибірки для забезпечення репрезентативності, використання метрик, які враховують дисбаланс (ROC-AUC, Average Precision), зважування класів у деяких моделях для збільшення важливості позитивних випадків, калібрування моделей для покращення якості ймовірностей. Дисбаланс класів враховується при інтерпретації результатів та виборі порогів для категорій ризику.

### 6.3. Моделі, які використовувались

У проєкті навчаються та оцінюються шість різних моделей машинного навчання для кожної цільової змінної, що дозволяє порівняти їх продуктивність та обрати найкращу. Кожна модель має свої переваги та недоліки, які роблять її підходящою для різних сценаріїв використання.

**Логістична регресія** — це лінійна модель класифікації, яка моделює ймовірність позитивного класу через логістичну функцію. Модель використовує лінійну комбінацію ознак з вагами, які навчаються для мінімізації функції втрат. Логістична регресія є однією з найпростіших та найінтерпретованіших моделей, що робить її ідеальною для медичних застосунків, де важливо розуміти, як модель приймає рішення.

**Для чого використовується** — логістична регресія використовується як базова модель для порівняння з більш складними алгоритмами та як основний інструмент для інтерпретації результатів. Модель дозволяє легко зрозуміти вплив кожної ознаки на прогноз через коефіцієнти, які показують, як зміна ознаки впливає на логарифм шансів позитивного класу.

**Переваги** — логістична регресія має кілька ключових переваг: висока інтерпретованість — коефіцієнти моделі можна легко інтерпретувати як вплив ознак на ризик, швидкість навчання та інференсу — модель навчається та передбачає дуже швидко, що важливо для реального використання, стабільність — модель не схильна до перенавчання на невеликих датасетах, простота — модель не має гіперпараметрів, які потребують налаштування, лінійність — модель припускає лінійні зв'язки між ознаками та цільовою змінною, що часто є достатнім для багатьох медичних задач.

**Чому вона стала основною** — логістична регресія часто стає основною моделлю для медичних застосунків через свою інтерпретованість та надійність. У проєкті вона може бути обрана як champion-модель, якщо показує найкращі результати за метриками ROC-AUC та Average Precision. Навіть якщо інші моделі показують трохи кращі результати, логістична регресія може бути обрана через свою простоту та зрозумілість для користувачів та медичних фахівців.

**Як інтерпретується результат** — результат логістичної регресії інтерпретується через ймовірність позитивного класу (0-1), яка показує ризик розвитку захворювання. Коефіцієнти моделі показують вплив кожної ознаки: позитивний коефіцієнт означає, що збільшення ознаки збільшує ризик, негативний — зменшує. Величина коефіцієнта показує силу впливу. Наприклад, якщо коефіцієнт для віку дорівнює 0.05, це означає, що збільшення віку на 1 рік збільшує логарифм шансів на 0.05, що відповідає приблизно 5% збільшенню ризику.

**Random Forest** — це ансамблева модель, яка об'єднує прогнози множини дерев рішень для отримання фінального результату. Кожне дерево навчається на випадковій підвибірці даних та випадковому підмножині ознак, що забезпечує різноманітність дерев та зменшує перенавчання. Фінальний прогноз обчислюється як середнє або мажоритарне голосування всіх дерев.

**Переваги** — Random Forest має кілька переваг: висока точність — модель часто показує кращі результати, ніж прості алгоритми, завдяки комбінації множини дерев, стійкість до викидів — модель менш чутлива до аномальних значень, ніж лінійні моделі, автоматичний відбір ознак — модель може використовувати велику кількість ознак та автоматично визначати найважливіші, обробка нелінійних зв'язків — модель може виявляти складні взаємодії між ознаками, вбудована оцінка важливості ознак — модель автоматично обчислює важливість кожної ознаки.

**Складність** — Random Forest є більш складною моделлю, ніж логістична регресія, що створює кілька викликів: менша інтерпретованість — важко зрозуміти, як модель приймає рішення через комбінацію множини дерев, більша складність налаштування — модель має гіперпараметри (кількість дерев, глибина дерев, мінімальна кількість зразків у листі), які потребують оптимізації, більша вимогливість до ресурсів — модель потребує більше пам'яті та часу для навчання та інференсу, ризик перенавчання — при неправильному налаштуванні модель може перенавчитися на тренувальних даних.

**Чому використовувався як порівняльна модель** — Random Forest використовується як порівняльна модель для оцінки, чи можуть складніші алгоритми покращити результати порівняно з простою логістичною регресією. Якщо Random Forest показує значно кращі результати, це може обґрунтувати використання більш складних моделей. Якщо різниця невелика, логістична регресія може бути обрана через свою простоту та інтерпретованість.

**Gradient Boosting (XGBoost)** — це потужна ансамблева модель, яка послідовно навчає слабкі моделі (зазвичай дерева рішень) для виправлення помилок попередніх моделей. Кожна нова модель фокусується на випадках, які попередні моделі передбачили неправильно, що дозволяє поступово покращувати загальну продуктивність.

**Чому тестувався** — XGBoost тестувався як один з найпотужніших алгоритмів машинного навчання, який часто показує найкращі результати на різних задачах класифікації. Модель має репутацію високої точності та здатності виявляти складні патерни в даних, що робить її привабливою для медичних застосунків, де точність критично важлива.

**Які результати показав** — XGBoost може показати найкращі результати за метриками ROC-AUC та Average Precision серед усіх тестованих моделей, що може зробити його champion-моделлю. Однак модель має високу складність та меншу інтерпретованість, що може бути компромісом між точністю та зрозумілістю. Результати залежать від конкретного датасету та налаштувань моделі.

**Інші моделі** — крім логістичної регресії, Random Forest та XGBoost, у проєкті також тестуються SVC (Support Vector Classifier), KNN (K-Nearest Neighbors) та MLP (Multi-Layer Perceptron, нейромережа). Кожна з цих моделей має свої особливості: SVC використовує ядра для обробки нелінійних зв'язків, KNN базується на подібності з найближчими сусідами, MLP використовує глибоке навчання для виявлення складних патернів. Всі моделі оцінюються за однаковими метриками, що дозволяє об'єктивно порівняти їх продуктивність та обрати найкращу.

### 6.4. Компонування моделей у проєкті

Моделі машинного навчання організовані у проєкті за чіткою структурою, яка забезпечує їх збереження, завантаження та використання в API для реального прогнозування.

**Файли ML-моделей у репозиторії** — всі навчені моделі зберігаються у директорії `artifacts/models/`, яка організована за цільовими змінними та назвами моделей. Структура директорії має вигляд `artifacts/models/{target}/{ModelName}/`, де `target` — це цільова змінна (`diabetes_present` або `obesity_present`), а `ModelName` — назва моделі (наприклад, `LogisticRegression`, `RandomForest`, `XGBoost`). Кожна модель має свою піддиректорію, яка містить файл моделі, метрики, графіки та інші артефакти.

**Де зберігаються pickle файли** — моделі зберігаються у форматі `.joblib` (бібліотека joblib для серіалізації Python-об'єктів), який є оптимізованою версією pickle для великих масивів NumPy. Файли моделей мають назву `model.joblib` та зберігаються у піддиректорії кожної моделі. Калібровані версії чемпіонських моделей зберігаються окремо як `champion_calibrated.joblib` у директорії цільової змінної. Моделі зберігають повний pipeline, який включає препроцесор (імпутацію, стандартизацію, кодування) та навчену модель, що дозволяє використовувати їх безпосередньо для прогнозування без додаткової підготовки даних.

**Як моделі завантажуються у API** — моделі завантажуються через систему реєстрації моделей, яка реалізована у модулі `model_registry.py`. При першому запиті на прогноз модель завантажується з диску у пам'ять за допомогою `joblib.load()`, після чого кешується у словнику `_MODEL_CACHE` для швидкого доступу при наступних запитах. Система віддає перевагу каліброваним моделям, якщо вони доступні, інакше використовує звичайну чемпіонську модель. Кешування забезпечує, що модель завантажується тільки один раз при першому використанні, що значно покращує продуктивність API.

**Структура функцій прогнозування** — функції прогнозування організовані як pipeline, який включає кілька кроків: валідація вхідних даних через Pydantic-схему для перевірки типів, діапазонів та обов'язковості полів; конвертація вхідних даних у pandas DataFrame з колонками, що відповідають ознакам моделі; передача DataFrame через pipeline моделі, який автоматично виконує імпутацію пропущених значень, стандартизацію числових ознак та кодування категоріальних; обчислення ймовірності позитивного класу через `pipeline.predict_proba()`; визначення категорії ризику на основі ймовірності (низький, середній, високий); обчислення топ факторів впливу через функцію `calculate_top_factors_simple()`; формування відповіді у форматі JSON для повернення клієнту.

### 6.5. Метрики

Оцінка продуктивності моделей виконується за множиною метрик, кожна з яких оцінює різні аспекти якості прогнозів. Використання кількох метрик забезпечує всебічну оцінку моделей та дозволяє обрати найкращу для конкретного застосування.

**Accuracy** — це найпростіша метрика, яка показує частку правильних прогнозів серед усіх прогнозів. Accuracy обчислюється як відношення кількості правильних прогнозів до загальної кількості прогнозів. Метрика легко інтерпретується та зрозуміла для широкої аудиторії, але має обмеження для незбалансованих датасетів, де модель може досягти високої точності, просто завжди передбачаючи більшість клас. Для датасету з 7.5% позитивних випадків діабету модель може досягти 92.5% точності, просто завжди передбачаючи відсутність діабету, що робить метрику неінформативною.

**ROC-AUC** — це основна метрика для бінарної класифікації, яка показує здатність моделі розрізняти позитивні та негативні класи незалежно від порогу класифікації. ROC-AUC обчислюється як площа під кривою ROC (Receiver Operating Characteristic), яка показує залежність між чутливістю (True Positive Rate) та специфічністю (False Positive Rate) при різних порогах. Значення ROC-AUC коливається від 0 до 1, де 1 означає ідеальну модель, а 0.5 — модель, яка працює не краще за випадкову. ROC-AUC є основною метрикою для вибору champion-моделі, оскільки вона враховує дисбаланс класів та оцінює здатність моделі розрізняти класи на всіх можливих порогах.

**Precision/Recall** — це метрики, які оцінюють якість позитивних прогнозів та здатність моделі виявляти позитивні випадки. Precision (точність) показує частку правильних позитивних прогнозів серед усіх позитивних прогнозів, тобто скільки з передбачених позитивних випадків дійсно є позитивними. Recall (повнота, чутливість) показує частку виявлених позитивних випадків серед усіх реальних позитивних випадків, тобто скільки позитивних випадків модель змогла виявити. Для медичних застосунків часто важливіше мати високий recall, щоб не пропустити реальні випадки захворювань, навіть якщо це призводить до деяких хибно-позитивних результатів.

**Чому саме ці метрики** — вибір саме цих метрик обумовлений специфікою задачі та характеристиками датасету. ROC-AUC обрана як основна метрика через свою здатність оцінювати модель незалежно від порогу класифікації та врахування дисбалансу класів. Average Precision обрана як додаткова метрика для незбалансованих класів, оскільки вона фокусується на якості позитивних прогнозів. Precision та Recall використовуються для детального аналізу якості прогнозів та виявлення слабких місць моделей. F1-score використовується як гармонійне середнє precision та recall для збалансованої оцінки. Brier Score використовується для оцінки калібрування ймовірностей, що критично важливо для медичних застосунків, де точність ймовірностей впливає на прийняття рішень.

**Інтерпретація** — інтерпретація метрик залежить від контексту застосування. Для медичних застосунків високий ROC-AUC (вище 0.8) вважається хорошим результатом, оскільки він показує, що модель значно краща за випадкову. Високий Precision означає, що коли модель передбачає наявність ризику, це дійсно так у більшості випадків, що важливо для уникнення зайвих тривог. Високий Recall означає, що модель виявляє більшість реальних випадків, що важливо для недопущення пропуску захворювань. Баланс між Precision та Recall залежить від конкретного застосування: для скринінгу важливіший Recall, для діагностики — Precision.

**Порівняння моделей** — порівняння моделей виконується за кількома метриками одночасно, що дозволяє обрати найкращу модель для конкретного застосування. Champion-модель обирається на основі найвищого ROC-AUC, а у разі рівності — найвищого Average Precision. Результати всіх моделей зберігаються у `leaderboard.csv`, який містить всі метрики для кожної моделі, що дозволяє порівняти їх продуктивність та зробити обґрунтований вибір. Порівняння також враховує інші фактори, такі як інтерпретованість, швидкість інференсу та складність моделі.

### 6.6. Інтерпретація моделей

Інтерпретація моделей є критично важливою для медичних застосунків, де користувачі та фахівці потребують розуміння того, як модель приймає рішення та які фактори найбільше впливають на прогноз.

**Feature importance** — важливість ознак обчислюється через permutation importance, який показує, наскільки знижується продуктивність моделі при випадковому перемішуванні значень ознаки. Метод працює наступним чином: модель передбачає на вибірці даних та обчислює базову метрику (наприклад, ROC-AUC); значення однієї ознаки випадково перемішуються, а інші залишаються незмінними; модель знову передбачає на модифікованих даних та обчислює метрику; різниця між базовою та модифікованою метрикою показує важливість ознаки. Чим більше знижується метрика при перемішуванні ознаки, тим важливіша ця ознака для моделі. Permutation importance обчислюється для champion-моделі під час навчання та зберігається у `champion_importance.json` для використання в API та веб-інтерфейсі.

**Як робиться факторний аналіз** — факторний аналіз виконується на двох рівнях: глобальний рівень (для всієї моделі) та індивідуальний рівень (для конкретного прогнозу). На глобальному рівні використовується permutation importance для визначення загальної важливості кожної ознаки для моделі. Ця інформація використовується в ендпоінті `/explain` для пояснення моделі в цілому. На індивідуальному рівні використовується спрощений підхід, який аналізує нормалізовані значення ознак для конкретного користувача та обчислює їх вплив на прогноз. Ця інформація використовується в ендпоінті `/predict` для показу топ факторів, які найбільше впливають на конкретний прогноз користувача.

**Чому деякі фічі важливіші** — важливість ознак залежить від кількох факторів: сила зв'язку з цільовою змінною — ознаки, які мають сильний зв'язок з цільовою змінною, мають більшу важливість; унікальність інформації — ознаки, які надають унікальну інформацію, яку не можна отримати з інших ознак, мають більшу важливість; варіативність — ознаки з великою варіативністю можуть мати більшу важливість, оскільки вони дозволяють краще розрізняти класи; взаємодії — деякі ознаки можуть мати велику важливість через взаємодії з іншими ознаками, навіть якщо індивідуально вони мають слабкий зв'язок. Наприклад, ІМТ може мати велику важливість для прогнозування ожиріння, оскільки він є прямим показником цього стану, а вік може мати велику важливість для діабету через його зв'язок з метаболічними змінами.

**Як це впливає на PDF-звіти та UI** — важливість ознак інтегрована у веб-інтерфейс та PDF-звіти для надання користувачам зрозумілої інформації про фактори ризику. У веб-інтерфейсі топ фактори відображаються після кожного прогнозування у вигляді списку з відсотками впливу, що дозволяє користувачам зрозуміти, які показники найбільше впливають на їхній ризик. У PDF-звітах топ фактори включаються як окремий розділ з детальним описом впливу кожного фактора, що дозволяє користувачам зберегти та проаналізувати цю інформацію. Назви ознак перекладаються на зрозумілі користувачу терміни (наприклад, `RIDAGEYR` → "Вік", `BMXBMI` → "ІМТ"), що робить інформацію доступною для широкої аудиторії. Вплив показується у відсотках або нормалізованих значеннях, що дозволяє легко порівняти важливість різних факторів. Фактори сортуються за спаданням впливу, що дозволяє користувачам швидко ідентифікувати найважливіші фактори ризику.

## 7. API

### 7.1. Концепція API

API (Application Programming Interface) проєкту HealthRisk.AI є центральним шаром взаємодії між фронтендом, ML-моделями, базою даних та системою прогнозування ризиків здоров'я. API забезпечує структуровану передачу даних, обробку запитів та повернення результатів у стандартизованому форматі.

**FastAPI як основа** — API побудований на FastAPI, сучасному асинхронному веб-фреймворку для Python, який забезпечує високу продуктивність, автоматичну генерацію OpenAPI-документації та вбудовану валідацію даних через Pydantic. FastAPI базується на Starlette для асинхронної обробки запитів та Pydantic для валідації та серіалізації даних, що робить його ідеальним вибором для ML-застосунків, де потрібна швидка обробка запитів та надійна валідація вхідних даних.

**Чому цей фреймворк** — FastAPI обрано для проєкту через кілька ключових переваг: висока продуктивність — асинхронна обробка запитів забезпечує швидкий відгук навіть при високому навантаженні; автоматична документація — OpenAPI-схема генерується автоматично, що спрощує розробку та тестування; типобезпека — Pydantic забезпечує автоматичну валідацію типів та значень, що зменшує кількість помилок; простота розробки — мінімальний boilerplate код та інтуїтивний синтаксис спрощують створення ендпоінтів; інтеграція з ML — легка інтеграція з бібліотеками машинного навчання та можливість використання синхронних функцій для ML-операцій.

**Модель request/response** — API використовує стандартну REST модель, де клієнт надсилає HTTP-запит з даними у форматі JSON, а сервер обробляє запит та повертає відповідь також у форматі JSON. Всі запити валідуються через Pydantic-схеми, які перевіряють наявність обов'язкових полів, типи даних та діапазони значень перед обробкою. Всі відповіді також структуровані через Pydantic-схеми, що забезпечує консистентність та передбачуваність формату даних.

**Формат JSON** — всі дані передаються у форматі JSON, що забезпечує сумісність з різними клієнтами та платформами. JSON є стандартним форматом для REST API та підтримується всіма сучасними мовами програмування та фреймворками. Структура JSON визначається через Pydantic-схеми, які автоматично серіалізують Python-об'єкти у JSON та десеріалізують JSON у Python-об'єкти, що спрощує роботу з даними.

**Продуктивність** — API оптимізований для високої продуктивності через кілька механізмів: кешування моделей — моделі завантажуються один раз та зберігаються в пам'яті для швидкого доступу; асинхронна обробка — FastAPI використовує асинхронну обробку запитів, що дозволяє обробляти кілька запитів одночасно; оптимізація запитів до БД — використання індексів та ефективних SQL-запитів забезпечує швидкий доступ до даних; мінімальна обробка — дані обробляються тільки необхідним чином без зайвих перетворень; локальне виконання — всі компоненти (API, БД, ML-моделі) працюють локально, що зменшує мережеві затримки.

### 7.2. Головні групи ендпоінтів

API організовано за функціональними групами ендпоінтів, кожна з яких виконує конкретні завдання та забезпечує специфічну функціональність системи.

**`/predict` — отримання прогнозу** — це основний ендпоінт системи, який приймає параметри пацієнта (вік, стать, ІМТ, артеріальний тиск, глюкоза, холестерин) та цільову змінну (діабет або ожиріння), завантажує відповідну ML-модель, виконує інференс та повертає ймовірність ризику, категорію ризику (низький, помірний, високий), топ фактори впливу та метадані моделі. Ендпоінт підтримує опційну автентифікацію — якщо користувач автентифікований, прогноз автоматично зберігається в історії для подальшого аналізу. Ендпоінт валідує вхідні дані через Pydantic-схему, обробляє пропущені значення через pipeline моделі та обчислює топ фактори впливу для інтерпретації результату.

**`/auth/*` — реєстрація, логін, токени** — група ендпоінтів для аутентифікації та управління користувачами включає `/auth/register` для реєстрації нового користувача з валідацією email, пароля та обов'язкових полів профілю; `/auth/login` для входу користувача з поверненням JWT-токена та профілю; `/auth/me` для отримання профілю поточного автентифікованого користувача; `/auth/update-profile` для оновлення профілю користувача (ім'я, прізвище, дата народження, стать, аватар); `/auth/change-password` для зміни пароля з перевіркою поточного пароля; `/auth/forgot-password` для ініціалізації процесу відновлення пароля через email-токен; `/auth/reset-password` для встановлення нового пароля через токен відновлення. Всі ендпоінти використовують хешування паролів через bcrypt для безпеки та валідацію даних через Pydantic-схеми.

**`/users/*` — робота з користувачами** — ендпоінти для управління користувачами включають `/users/history` для отримання історії прогнозів користувача з можливістю фільтрації та пагінації; `/users/history/stats` для статистики історії прогнозів для побудови діаграм (розподіл по цілям, категоріям ризику, моделям, часова серія); `/users/history/{id}` для видалення конкретного прогнозу з історії; `/users/history/clear` для очищення всієї історії прогнозів користувача; `/users/avatar` для завантаження аватару користувача з валідацією формату та розміру. Всі ендпоінти вимагають автентифікації та перевіряють належність ресурсів користувачу.

**`/chats/*` — чат і AI-асистент** — ендпоінти для системи чатів між користувачами включають `/api/chats` для отримання списку всіх чатів користувача з інформацією про останнє повідомлення та кількість непрочитаних; `/api/chats/{chat_uuid}` для отримання детальної інформації про конкретний чат з усіма повідомленнями; `/api/chats` (POST) для створення нового чату або отримання існуючого між двома користувачами; `/api/chats/{chat_uuid}/messages` (POST) для відправки повідомлення в чат; `/api/chats/{chat_uuid}/read` (POST) для позначення всіх повідомлень у чаті як прочитаних; `/api/chats/{chat_uuid}` (DELETE) для видалення чату та всіх його повідомлень; `/api/chats/{chat_uuid}/pin` (PATCH) для закріплення або відкріплення чату; `/api/chats/reorder` (PATCH) для зміни порядку чатів у списку; `/api/chats/users` для отримання списку всіх активних користувачів для створення чатів; `/api/chats/unread-count` для отримання загальної кількості непрочитаних повідомлень. Всі ендпоінти перевіряють блокування користувачів та доступ до ресурсів.

**`/reports/*` — генерація PDF** — API не має спеціальних ендпоінтів для генерації PDF, оскільки PDF генерується на фронтенді. Однак API надає дані для генерації звітів через ендпоінти `/users/history` для отримання історії прогнозів, `/users/history/stats` для статистики для діаграм та `/predict` для результатів прогнозування. Фронтенд використовує ці дані для генерації PDF-звітів через jsPDF та Chart.js.

**`/api-status/*` — статуси системи** — ендпоінти для моніторингу стану системи включають `/health` для перевірки стану API, повертає список доступних маршрутів та версію сервісу; `/system/database/stats` для отримання статистики бази даних, включаючи кількість записів у кожній таблиці, розмір БД та активність за останні 7 днів; `/assistant/health` для перевірки статусу Ollama сервера з вимірюванням латентності. Всі ендпоінти не вимагають автентифікації та використовуються для моніторингу доступності компонентів системи.

**`/history/*` — історія прогнозів** — ендпоінти для роботи з історією прогнозів включають `/users/history` для отримання списку всіх збережених прогнозів користувача з можливістю фільтрації та пагінації; `/users/history/stats` для статистики історії прогнозів для побудови діаграм; `/users/history/{id}` для видалення конкретного прогнозу з історії; `/users/history/clear` для очищення всієї історії прогнозів користувача. Всі ендпоінти вимагають автентифікації та перевіряють належність ресурсів користувачу.

### 7.3. Перевірка токенів і аутентифікація

Система аутентифікації API побудована на JWT (JSON Web Tokens) та забезпечує безпечний доступ до захищених ресурсів через механізм токенів.

**JWT** — JSON Web Tokens є стандартом для безпечної передачі інформації між сторонами у форматі JSON. Токени містять закодовану інформацію про користувача (email, час закінчення дії) та підписані секретним ключем для забезпечення цілісності. JWT використовується для автентифікації користувачів без необхідності зберігати стан сесії на сервері, що робить API stateless та масштабованим.

**Access token** — це основний токен для автентифікації, який видається після успішного входу або реєстрації. Токен містить email користувача та час закінчення дії (за замовчуванням 60 хвилин) та підписується секретним ключем через алгоритм HS256. Токен передається у заголовку `Authorization` у форматі `Bearer <token>` при кожному запиті до захищених ендпоінтів. API перевіряє валідність токена, розшифровує його та завантажує користувача з БД для подальшої обробки запиту.

**Refresh token** — наразі не реалізований у проєкті, але може бути доданий для продовження сесії без повторного входу. Refresh токени мають більший термін дії (наприклад, 7 днів) та зберігаються в БД для можливості відкликання. Коли access token закінчується, клієнт може використати refresh token для отримання нового access token без необхідності повторного входу.

**Чому така схема** — вибір JWT-схеми обумовлений кількома факторами: stateless — API не зберігає стан сесії, що робить його масштабованим та легким для розгортання; безпека — токени підписані секретним ключем, що забезпечує їх цілісність та автентичність; простота — токени легко передаються через HTTP-заголовки та не потребують складних механізмів зберігання; масштабованість — токени можуть бути перевірені на будь-якому сервері без доступу до БД, що дозволяє горизонтально масштабувати API.

**Як frontend використовує токени** — фронтенд зберігає JWT-токен у LocalStorage під ключем `hr_auth_token` після успішного входу або реєстрації. Токен автоматично додається до всіх API-запитів у заголовку `Authorization` у форматі `Bearer <token>`. Фронтенд перевіряє наявність токена при завантаженні сторінки та автоматично перенаправляє на сторінку входу, якщо токен відсутній або невалідний. При отриманні помилки 401 (неавторизований) фронтенд автоматично очищає токен та перенаправляє користувача на сторінку входу.

**Як захищені маршрути обробляються на API** — захищені маршрути використовують dependency injection через `Depends(require_current_user)`, який автоматично витягує токен з заголовка `Authorization`, перевіряє його валідність, розшифровує та завантажує користувача з БД. Якщо токен невалідний або користувач не знайдений, функція викидає HTTPException зі статусом 401, що призводить до повернення помилки клієнту. Якщо токен валідний, користувач передається у функцію ендпоінта як параметр, що дозволяє використовувати його для обробки запиту та перевірки доступу до ресурсів.

### 7.4. Як API взаємодіє з ML-моделями

API інтегрується з ML-моделями через систему реєстрації моделей, яка забезпечує їх завантаження, кешування та використання для інференсу.

**Як модель завантажується у пам'ять** — моделі завантажуються через функції `load_champion()` та `load_model()` у модулі `model_registry.py`. При першому запиті на прогноз модель завантажується з диску у пам'ять за допомогою `joblib.load()`, який десеріалізує збережений pipeline (препроцесор + модель) з файлу `.joblib`. Після завантаження модель зберігається у словнику `_MODEL_CACHE` для швидкого доступу при наступних запитах. Система віддає перевагу каліброваним моделям (`champion_calibrated.joblib`), якщо вони доступні, інакше використовує звичайну чемпіонську модель.

**Як вона використовується при кожному запиті** — при кожному запиті на прогноз API виконує наступні кроки: отримує вхідні дані користувача через Pydantic-схему `PredictRequest`; валідує дані (типи, діапазони, обов'язковість полів); завантажує модель через `load_champion()` або `load_model()` (з кешу, якщо вже завантажена); конвертує вхідні дані у pandas DataFrame з колонками, що відповідають ознакам моделі; передає DataFrame через pipeline моделі, який автоматично виконує імпутацію пропущених значень, стандартизацію числових ознак та кодування категоріальних; обчислює ймовірність позитивного класу через `pipeline.predict_proba()`; визначає категорію ризику на основі ймовірності; обчислює топ фактори впливу; формує відповідь у форматі JSON та повертає клієнту.

**Чому не можна завантажувати при кожному запиті** — завантаження моделі при кожному запиті було б неефективним через кілька причин: час завантаження — завантаження моделі з диску займає кілька секунд, що робить API повільним та непридатним для реального використання; використання пам'яті — повторне завантаження моделі призводить до дублювання даних у пам'яті, що збільшує використання ресурсів; навантаження на диск — часті операції читання з диску створюють додаткове навантаження на систему та зменшують продуктивність; нестабільність — завантаження моделі при кожному запиті може призвести до нестабільності через можливі помилки читання з диску. Кешування моделей у пам'яті забезпечує швидкий доступ (мілісекунди замість секунд) та стабільність роботи API.

**Робота з FeatureImpact** — обчислення топ факторів впливу виконується через функцію `calculate_top_factors_simple()`, яка аналізує нормалізовані значення ознак для конкретного користувача та обчислює їх вплив на прогноз. Функція використовує нормалізовані абсолютні значення ознак як проксі для впливу, оскільки точне обчислення впливу для складних моделей (наприклад, Random Forest, XGBoost) потребує значно більше обчислень. Фактори ранжуються за абсолютним значенням впливу та повертаються як топ-5 найважливіших. Для детального пояснення моделі використовується ендпоінт `/explain`, який обчислює permutation importance на вибірці з датасету для глобального пояснення моделі.

**Алгоритм повернення JSON-відповіді** — формування JSON-відповіді виконується автоматично через Pydantic-схему `PredictResponse`, яка серіалізує дані у JSON-формат. Відповідь включає поля `target` (цільова змінна), `probability` (ймовірність від 0 до 1), `risk_bucket` (категорія ризику: low, medium, high), `model_name` (назва моделі), `version` (версія моделі), `is_calibrated` (чи використовується калібрована модель), `top_factors` (список факторів з полями `feature` та `impact`), `note` (примітка про методику розрахунку). Ймовірність обмежується діапазоном (0.0001, 0.9999) для захисту від екстремальних значень та стабільності категорій ризику. Якщо користувач автентифікований, результат також зберігається в історії прогнозів через `save_history_entry()`.

### 7.5. Обробка помилок

Обробка помилок у API забезпечує надійність системи та надає користувачам зрозумілі повідомлення про проблеми.

**Перевірка введених даних** — всі вхідні дані валідуються через Pydantic-схеми, які перевіряють наявність обов'язкових полів, типи даних, діапазони значень та формати. При помилках валідації Pydantic автоматично генерує детальні повідомлення про помилки, які включають інформацію про поле, тип помилки та очікуване значення. Валідація виконується автоматично при отриманні запиту, що забезпечує, що тільки валідні дані передаються до обробки.

**Обробка 400, 401, 422** — API використовує стандартні HTTP статус-коди для індикації різних типів помилок: 400 (Bad Request) — невалідний запит, наприклад, відсутні обов'язкові поля або некоректні типи даних; 401 (Unauthorized) — неавторизований доступ, наприклад, відсутній або невалідний JWT-токен; 422 (Unprocessable Entity) — помилка валідації даних, наприклад, значення поза дозволеним діапазоном або некоректний формат. Всі помилки повертаються у форматі JSON з полем `detail`, яке містить детальне опис помилки для зручності обробки на фронтенді.

**Валідація полів** — валідація полів виконується на кількох рівнях: рівень Pydantic — автоматична валідація типів, діапазонів та обов'язковості полів через Pydantic-схеми; рівень бізнес-логіки — додаткова валідація специфічних правил, наприклад, перевірка унікальності email при реєстрації або перевірка належності ресурсів користувачу; рівень БД — перевірка обмежень БД, наприклад, унікальність ключів або foreign key constraints. Багаторівнева валідація забезпечує надійність системи та запобігає обробці некоректних даних.

**Стандартизовані помилки** — всі помилки повертаються у стандартизованому форматі JSON з полями `detail` (детальний опис помилки) та опційними полями для додаткової інформації. Формат помилок узгоджений по всьому API, що спрощує обробку на фронтенді та забезпечує передбачуваність поведінки. Помилки також логуються на сервері для діагностики та моніторингу проблем.

**Валідація через Pydantic** — Pydantic забезпечує автоматичну валідацію даних через визначення схем, які описують очікувану структуру та типи даних. Схеми автоматично перевіряють типи (int, float, str, bool), діапазони значень (наприклад, вік 0-120, ІМТ 10-60), обов'язковість полів (required vs optional), формати (наприклад, email, UUID), та валідність значень (наприклад, enum values). При помилках валідації Pydantic автоматично генерує детальні повідомлення про помилки, які включають інформацію про поле, тип помилки та очікуване значення, що спрощує діагностику проблем на стороні клієнта.

### 7.6. PDF-звіти

Генерація PDF-звітів у проєкті реалізована переважно на фронтенді, а API забезпечує передачу необхідних даних у форматі JSON.

**Як API взаємодіє/не взаємодіє з PDF** — API не генерує PDF безпосередньо, а надає дані через стандартні ендпоінти у форматі JSON. API не має спеціальних ендпоінтів для генерації PDF, оскільки вся логіка форматування та візуалізації знаходиться на клієнті. API забезпечує консистентність даних через Pydantic-схеми, але не визначає формат звіту, що дає фронтенду гнучкість у виборі способу відображення даних.

**Чому PDF на фронтенді** — генерація PDF на фронтенді має кілька переваг: зменшення навантаження на сервер — генерація PDF виконується на клієнті, що зменшує навантаження на сервер та дозволяє обробляти більше запитів; швидкість — генерація PDF на клієнті забезпечує швидкий відгук без необхідності очікування обробки на сервері; гнучкість — фронтенд може легко змінювати формат звітів без змін на сервері; масштабованість — генерація PDF на клієнті не обмежує кількість одночасних запитів на генерацію звітів. Недоліком є залежність від можливостей браузера та необхідність завантаження додаткових бібліотек на клієнті.

**Як дані передаються** — дані для генерації PDF передаються через стандартні API ендпоінти у форматі JSON. Фронтенд робить запити до `/users/history` для отримання історії прогнозів, `/users/history/stats` для статистики для діаграм та `/predict` для результатів прогнозування. Всі відповіді повертаються у форматі JSON з структурованими даними, що дозволяє фронтенду легко обробляти їх для генерації звітів. API не виконує серверного рендерингу — вся логіка форматування та візуалізації знаходиться на клієнті.

### 7.7. API для AI-асистента

API інтегрується з локальним Ollama сервером для реалізації AI-асистента здоров'я, який надає персоналізовані рекомендації користувачам.

**Виклики Ollama** — API виконує HTTP POST-запити до локального Ollama сервера (`http://localhost:11434/api/generate`) з використанням бібліотеки `requests`. Запити включають модель (за замовчуванням `llama3`), промпт з інструкціями та контекстом, параметри генерації (temperature, max_tokens) та опційні параметри для контролю поведінки моделі. API не підтримує streaming — всі запити виконуються синхронно з отриманням повної відповіді, що спрощує обробку, але може призводити до затримок при довгих відповідях.

**Обробка контексту** — контекст про стан здоров'я користувача формується на основі останнього прогнозу через функцію `build_health_context()`, яка включає інформацію про цільову змінну (діабет або ожиріння), ймовірність ризику, категорію ризику (низький, середній, високий), топ фактори впливу та вхідні параметри користувача. Контекст формується українською або англійською мовою залежно від параметра `language` у запиті. Контекст додається до системного промпту, який містить інструкції для асистента (не ставити діагнози, не призначати лікування, надавати загальні рекомендації) та забезпечує, що асистент надає корисні та безпечні відповіді.

**Логіка message history** — історія повідомлень зберігається в БД у таблиці `assistantmessage` з полями `user_id`, `role` (user або assistant), `content` (текст повідомлення), `prediction_id` (опційний зв'язок з прогнозом), `created_at` (timestamp). При відправці повідомлення користувача воно зберігається в БД, після чого формується контекст, викликається Ollama, відповідь асистента зберігається в БД та повертається користувачу. Історія повідомлень може бути отримана через ендпоінт `/assistant/history` для відображення на фронтенді та використання в подальших діалогах для забезпечення контексту розмови.

