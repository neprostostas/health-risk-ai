# Тестування в проєкті HealthRisk.AI

Система тестування проєкту HealthRisk.AI організована за принципом багаторівневої архітектури, що забезпечує перевірку коректності логіки, стабільності API, роботи ML моделей, UI та інтеграцій на різних рівнях абстракції.

## Загальний огляд

**Рівні тестування** — у проєкті реалізовано декілька рівнів тестування для забезпечення якості коду та функціональності системи:
- **Unit тести** — тестування окремих функцій, утиліт, валідації даних та невеликих сервісних класів ізольовано від інших компонентів
- **Інтеграційні тести** — тестування взаємодії між компонентами (API та база даних, API та ML моделі, повний цикл запиту прогнозу)
- **End-to-end (e2e) тести** — тестування повних сценаріїв користувача від реєстрації до отримання результатів
- **ML тести** — тестування машинного навчання моделей, їх завантаження, стабільності передбачень, відповідності формату інпутів та аутпутів
- **Експериментальні тести** — тестування екстремальних значень, пропущених даних, чутливості моделей до змін факторів

**Призначення тестів** — тести перевіряють коректність логіки бізнес-процесів (аутентифікація, прогнозування, збереження історії), стабільність API (коректність відповідей, обробка помилок, валідація даних), роботу ML моделей (завантаження, передбачення, діапазони ймовірностей), UI компонентів (якщо є тести) та інтеграції між компонентами (API-БД, API-ML, API-Ollama).

**Фізичне розташування тестів** — всі тести розміщені у кореневій директорії `tests/` відносно кореня проєкту. Структура організована за типами тестів та компонентами системи для зручності навігації та запуску окремих груп тестів.

## Структура тестового каталогу

Тестовий каталог організований за принципом розділення за типами тестів та компонентами системи.

**Коренева папка `tests/`** містить всі тести проєкту, загальні фікстури (`conftest.py`), утиліти для генерації тестових даних (`utils/`) та підпапки для різних типів тестів.

**Підпапка `tests/backend/`** містить тести для бекенду, розділені на:
- **`tests/backend/unit/`** — unit тести для бекенду, які тестують окремі функції та утиліти (хешування паролів, JWT токени, валідація Pydantic схем) ізольовано від інших компонентів
- **`tests/backend/integration/`** — інтеграційні тести для бекенду, які тестують взаємодію між компонентами (автентифікація через API, прогнозування через API, збереження в БД)
- **`tests/backend/e2e/`** — end-to-end тести для бекенду, які тестують повні сценарії користувача (реєстрація → логін → прогноз → історія → оновлення профілю)
- **`tests/backend/experimental/`** — експериментальні тести для бекенду (наразі порожня, але структура готова для майбутніх тестів)

**Підпапка `tests/frontend/`** містить тести для фронтенду, розділені на:
- **`tests/frontend/unit/`** — unit тести для фронтенду (наразі містить тільки placeholder тест, структура готова для майбутніх тестів)
- **`tests/frontend/integration/`** — інтеграційні тести для фронтенду (наразі порожня, структура готова)
- **`tests/frontend/e2e/`** — end-to-end тести для фронтенду (наразі порожня, структура готова)

**Підпапка `tests/ml/`** містить тести для машинного навчання, розділені на:
- **`tests/ml/unit/`** — unit тести для ML, які тестують завантаження моделей, логіку прогнозування, діапазони ймовірностей, детермінізм передбачень
- **`tests/ml/experimental/`** — експериментальні тести для ML, які тестують екстремальні значення, пропущені дані, чутливість моделей до змін факторів
- **`tests/ml/metrics/`** — тести для метрик моделей (наразі порожня, структура готова)

**Підпапка `tests/utils/`** містить утиліти для тестів:
- **`tests/utils/fixtures.py`** — додаткові фікстури для тестів (генерація випадкових email, екстремальні дані, дані з пропущеними значеннями)
- **`tests/utils/test_data_generators.py`** — генератори тестових даних для різних сценаріїв (реалістичні дані, екстремальні випадки, дані з пропущеними значеннями, варіації для тестування чутливості)

**Файл `tests/conftest.py`** містить загальні фікстури для всіх тестів, включаючи створення тестової БД, тестового клієнта FastAPI, тестових даних користувача та прогнозування, заголовків автентифікації.

## Інструменти та фреймворки тестування

Проєкт використовує сучасні інструменти тестування для Python та підтримку асинхронних тестів.

**Pytest** — основний тестовий фреймворк для Python, який використовується для всіх тестів у проєкті. Pytest забезпечує автоматичне знаходження тестів, зручний синтаксис для написання тестів, фікстури для підготовки середовища, маркери для категорізації тестів, детальний вивід результатів та підтримку паралельного запуску тестів. Конфігурація pytest знаходиться у файлі `pytest.ini` з налаштуваннями шляхів, патернів файлів, маркерів та виводу.

**Pytest-asyncio** — додаткова бібліотека для підтримки асинхронних тестів у pytest. Використовується для тестування асинхронних ендпоінтів FastAPI, асинхронних операцій з БД та інших async/await функцій. Налаштований режим `asyncio_mode = auto` у `pytest.ini` для автоматичної обробки асинхронних тестів.

**Pytest-cov** — бібліотека для вимірювання покриття коду тестами (code coverage). Використовується для генерації звітів про покриття коду, виявлення непокритих частин коду та відстеження якості тестів. Запускається через команду `make test-coverage` з генерацією HTML-звіту у `htmlcov/index.html`.

**Httpx** — HTTP клієнт для тестування FastAPI додатків. Використовується для формування HTTP-запитів до API ендпоінтів у тестах, перевірки статус-кодів, структури JSON-відповідей та обробки помилок. Httpx інтегрується з FastAPI через `TestClient` для швидкого тестування без запуску реального сервера.

**Frontend тестові інструменти** — наразі фронтенд тести не реалізовані (є тільки placeholder тест у `tests/frontend/unit/test_placeholder.py`). Структура готова для майбутнього додавання тестів через Jest, Vitest, Cypress або інші інструменти, але наразі вони не використовуються.

## Unit тести

Unit тести перевіряють окремі функції та утиліти ізольовано від інших компонентів системи.

**Модулі, що покриваються unit тестами** включають:
- **Функції з бізнес-логіки** — хешування паролів, перевірка паролів, генерація JWT токенів, декодування токенів, обчислення ризиків, топ факторів
- **Утиліти** — конвертери даних, форматування, обробка помилок, валідація
- **Pydantic схеми** — валідація вхідних та вихідних даних API, перевірка типів, діапазонів, обов'язковості полів
- **Невеликі сервісні класи** — реєстр моделей, завантаження моделей, обробка даних

**Розташування unit тестів**:
- **`tests/backend/unit/test_auth_utils.py`** — тести для функцій аутентифікації (хешування паролів, JWT токени)
- **`tests/backend/unit/test_schemas.py`** — тести для Pydantic схем (валідація PredictRequest, PredictResponse, UserRegisterRequest, UserLoginRequest)
- **`tests/ml/unit/test_model_loading.py`** — тести для завантаження моделей (чемпіонські моделі, калібровані моделі, конкретні моделі за ключем)
- **`tests/ml/unit/test_prediction_logic.py`** — тести для логіки прогнозування (діапазони ймовірностей, детермінізм, реалістичні дані)

**Типові сценарії, що перевіряються**:
- **Коректні дані** — тести перевіряють, що функції коректно обробляють валідні вхідні дані та повертають очікувані результати (наприклад, хешування пароля повертає хеш у форматі bcrypt, валідація валідного email проходить успішно)
- **Edge cases** — тести перевіряють граничні випадки (наприклад, мінімальні/максимальні значення, порожні рядки, None значення, крайні діапазони)
- **Некоректні дані, очікувані винятки** — тести перевіряють, що функції коректно обробляють некоректні дані та викидають очікувані винятки (наприклад, невалідний email викликає ValidationError, неправильний пароль не проходить перевірку, прострочений токен викликає HTTPException)

## Інтеграційні тести

Інтеграційні тести перевіряють взаємодію між компонентами системи.

**Компоненти, що перевіряють інтеграційні тести**:
- **Взаємодія API і бази даних** — тести перевіряють, що API коректно зберігає дані в БД, читає дані з БД, оновлює записи, видаляє записи, обробляє помилки БД (наприклад, дублювання email, неіснуючі записи)
- **Інтеграція API з ML моделями** — тести перевіряють, що API коректно завантажує моделі, виконує прогнозування, обробляє вхідні дані, повертає результати у правильному форматі, зберігає історію прогнозів
- **Повний цикл запиту прогнозу** — тести перевіряють весь процес від отримання вхідних даних через API до збереження результату в БД, включаючи валідацію, обробку, прогнозування, збереження

**Розташування інтеграційних тестів**:
- **`tests/backend/integration/test_auth_flow.py`** — тести для автентифікації (реєстрація, логін, профіль, зміна пароля, дублювання email, невалідні дані)
- **`tests/backend/integration/test_predictions_api.py`** — тести для API прогнозування (прогноз діабету, прогноз ожиріння, валідація даних, обробка помилок, збереження в історії)

**Організація підготовки середовища**:
- **Використання тестової БД** — кожен тест використовує окрему тимчасову SQLite базу даних, створену через фікстуру `test_db` у `conftest.py`. База даних створюється в тимчасовому файлі, всі таблиці створюються автоматично через `SQLModel.metadata.create_all()`, після завершення тесту файл видаляється. Це забезпечує ізоляцію тестів та чисте середовище для кожного тесту.
- **Фікстури для створення користувача, токена, даних** — фікстури у `conftest.py` та `tests/utils/fixtures.py` забезпечують готові дані для тестів: `sample_user_data` (тестові дані користувача), `sample_prediction_data` (тестові дані для прогнозування діабету), `sample_prediction_data_obesity` (тестові дані для прогнозування ожиріння), `auth_headers` (заголовки з JWT токеном після реєстрації та входу), `extreme_prediction_data` (екстремальні дані), `minimal_prediction_data` (мінімальні дані), `missing_data_prediction` (дані з пропущеними значеннями)
- **Тестовий клієнт FastAPI** — фікстура `client` у `conftest.py` створює тестовий клієнт FastAPI з підміною БД через `app.dependency_overrides`, що дозволяє тестам використовувати тестову БД замість реальної. Після завершення тесту підміни очищаються через `app.dependency_overrides.clear()`

**Роль фікстур pytest** — фікстури забезпечують підготовку середовища для тестів (створення БД, клієнта, даних), автоматичне очищення після завершення тестів, повторне використання коду між тестами, параметризацію тестів через передачу різних даних у фікстурах. Фікстури з `scope="function"` створюються для кожного тесту окремо, що забезпечує ізоляцію.

## Тестування ML моделей

Тестування ML моделей перевіряє коректність завантаження моделей, стабільність передбачень, відповідність формату інпутів та аутпутів та якість метрик.

**Розташування ML тестів**:
- **`tests/ml/unit/test_model_loading.py`** — тести для завантаження моделей (чемпіонські моделі для діабету та ожиріння, калібровані моделі, конкретні моделі за ключем, обробка помилок при відсутності моделей)
- **`tests/ml/unit/test_prediction_logic.py`** — тести для логіки прогнозування (діапазони ймовірностей 0-1, детермінізм передбачень, реалістичні дані, сума ймовірностей = 1)
- **`tests/ml/experimental/test_extreme_values.py`** — тести для екстремальних значень (максимальний вік 120, максимальний ІМТ 60, максимальний тиск 250, максимальна глюкоза 400, мінімальні значення, перевірка що модель не падає та повертає валідні результати)
- **`tests/ml/experimental/test_missing_data.py`** — тести для пропущених даних (обробка None значень, імпутація через pipeline, стабільність передбачень при пропущених опціональних полях)
- **`tests/ml/experimental/test_sensitivity.py`** — тести для чутливості моделей (чутливість до зміни ІМТ, глюкози, віку, перевірка що збільшення факторів ризику збільшує ймовірність)

**Аспекти, що перевіряються**:
- **Чи модель не падає на краєвих значеннях параметрів** — тести перевіряють, що модель коректно обробляє екстремальні значення (максимальні/мінімальні вік, ІМТ, тиск, глюкоза, холестерин) без винятків, повертає валідні результати (не NaN, не Inf), обробляє нереалістичні комбінації параметрів
- **Чи модель повертає ймовірність у діапазоні [0,1]** — тести перевіряють, що `predict_proba()` повертає ймовірності в діапазоні 0-1, сума ймовірностей для всіх класів дорівнює 1, ймовірності не містять NaN або Inf значень
- **Чи модель детерміністична** — тести перевіряють, що однакові вхідні дані дають однакові результати при повторних викликах, що важливо для відтворюваності результатів
- **Чи модель коректно обробляє пропущені дані** — тести перевіряють, що pipeline коректно обробляє None значення через імпутацію, що передбачення стабільні при пропущених опціональних полях, що обов'язкові поля перевіряються до передачі в модель

**Маркери для ML тестів** — тести, які потребують навчених моделей, помічені маркером `@pytest.mark.skipif` з перевіркою наявності файлів моделей. Якщо моделі не знайдено, тести пропускаються з повідомленням про необхідність навчання моделей. Це дозволяє запускати тести навіть якщо моделі ще не навчені, але перевіряти їх при наявності моделей.

## Тестування API

Тестування API перевіряє коректність роботи ендпоінтів, обробку запитів, валідацію даних та обробку помилок.

**Ендпоінти, що покриваються тестами**:
- **Логін, реєстрація** — тести перевіряють успішну реєстрацію, логін з правильними credentials, обробку дублювання email, невалідних email, порожніх паролів, неправильних credentials, неактивних акаунтів
- **Прогноз ризику** — тести перевіряють прогноз діабету, прогноз ожиріння, валідацію вхідних даних, обробку відсутніх обов'язкових полів, діапазони ймовірностей, категорії ризику, топ фактори, збереження в історії
- **Історія** — тести перевіряють отримання історії прогнозів, фільтрацію за користувачем, сортування за датою, обмеження кількості записів
- **Чати** — тести можуть перевіряти створення чатів, відправку повідомлень, отримання історії чатів (якщо реалізовано)
- **API-status** — тести можуть перевіряти статус API, БД, Ollama (якщо реалізовано)

**Як тести формують HTTP запити** — тести використовують `TestClient` з FastAPI для формування HTTP-запитів без запуску реального сервера. Запити формуються через методи `client.get()`, `client.post()`, `client.put()`, `client.delete()` з передачею URL, JSON-даних, заголовків автентифікації. Тести використовують фікстуру `client` з `conftest.py`, яка створює тестовий клієнт з підміною БД.

**Як тести перевіряють статус код** — тести перевіряють статус-коди відповідей через `assert response.status_code == 200` для успішних запитів, `assert response.status_code == 400` для помилок валідації, `assert response.status_code == 401` для неавтентифікованих запитів, `assert response.status_code == 403` для відсутності прав, `assert response.status_code == 404` для неіснуючих ресурсів, `assert response.status_code == 409` для конфліктів (наприклад, дублювання email), `assert response.status_code == 422` для помилок валідації Pydantic.

**Як тести перевіряють структуру JSON-відповіді** — тести перевіряють структуру JSON через `response.json()` для отримання даних, `assert "field" in data` для перевірки наявності полів, `assert data["field"] == expected_value` для перевірки значень, `assert 0 <= data["probability"] <= 1` для перевірки діапазонів, `assert data["risk_bucket"] in ["low", "medium", "high"]` для перевірки допустимих значень.

**Як тести перевіряють сценарії з помилками**:
- **401 (Unauthorized)** — тести перевіряють, що запити без токена або з невалідним токеном повертають 401, що повідомлення про помилку коректне, що заголовок `WWW-Authenticate: Bearer` присутній
- **403 (Forbidden)** — тести перевіряють, що запити з неактивним акаунтом повертають 403, що спроби доступу до чужих даних повертають 403, що блокування користувачів працює коректно
- **422 (Unprocessable Entity)** — тести перевіряють, що невалідні дані (невалідний email, відсутні обов'язкові поля, некоректні типи, значення поза діапазоном) викликають 422, що повідомлення про помилки валідації детальні та зрозумілі

## Тестування фронтенду

Наразі фронтенд тести не реалізовані — у проєкті є тільки placeholder тест у `tests/frontend/unit/test_placeholder.py`, який завжди проходить та підтверджує, що структура для тестів фронтенду готова. Структура папок (`tests/frontend/unit/`, `tests/frontend/integration/`, `tests/frontend/e2e/`) створена для майбутнього додавання тестів, але наразі тести не написані.

**Майбутні інструменти для фронтенду** можуть включати Jest або Vitest для unit тестів JavaScript коду, Cypress або Playwright для e2e тестів браузера, але наразі вони не використовуються та не налаштовані.

## Експериментальні тести та нестандартні сценарії

Експериментальні тести перевіряють нестандартні сценарії та граничні випадки для підтвердження надійності системи.

**Розташування експериментальних тестів**:
- **`tests/ml/experimental/test_extreme_values.py`** — тести для екстремальних значень параметрів (максимальний вік 120, максимальний ІМТ 60, максимальний тиск 250, максимальна глюкоза 400, мінімальні значення, нереалістичні комбінації)
- **`tests/ml/experimental/test_missing_data.py`** — тести для пропущених даних (один пропущений параметр, кілька пропущених параметрів, всі опціональні поля пропущені, обробка через імпутацію)
- **`tests/ml/experimental/test_sensitivity.py`** — тести для чутливості моделей до змін факторів (чутливість до зміни ІМТ, глюкози, віку, перевірка що збільшення факторів ризику збільшує ймовірність)

**Що перевіряють експериментальні тести**:
- **Екстремальні значення** — тести перевіряють, що модель не падає на граничних значеннях параметрів, повертає валідні результати (не NaN, не Inf), обробляє нереалістичні комбінації параметрів (наприклад, дитина з параметрами дорослого, дорослий з параметрами дитини)
- **Пропущені дані** — тести перевіряють, що pipeline коректно обробляє None значення через імпутацію, що передбачення стабільні при пропущених опціональних полях, що обов'язкові поля перевіряються до передачі в модель
- **Чутливість моделей** — тести перевіряють, що збільшення факторів ризику (ІМТ, глюкоза, вік) збільшує ймовірність позитивного класу, що модель реагує на зміни параметрів логічно та передбачувано

**Як допомагають підтвердити надійність системи** — експериментальні тести допомагають виявити проблеми з обробкою граничних випадків, перевірити стабільність моделей на нестандартних даних, підтвердити що система не падає при неочікуваних вхідних даних, забезпечити що модель повертає валідні результати навіть при екстремальних значеннях, перевірити що обробка пропущених даних працює коректно.

**Майбутні експериментальні тести** можуть включати стрес-тести для ML моделей (велика кількість одночасних запитів), тести з псевдорандомними даними для перевірки стабільності, тести з некоректними типами даних, тести з дуже великими обсягами даних, тести продуктивності (час відповіді, використання пам'яті).

## Запуск тестів та конфігурація

Запуск тестів організований через Makefile з окремими командами для різних типів тестів.

**Як запустити всі тести одним рядком** — команда `make test` або `pytest tests/` запускає всі тести у проєкті. Pytest автоматично знаходить всі тестові файли за патерном `test_*.py` у директорії `tests/` та запускає їх з налаштуваннями з `pytest.ini`.

**Окремі задачі для різних типів тестів**:
- **`make test-backend`** або `pytest tests/backend/` — запускає всі тести бекенду (unit, integration, e2e)
- **`make test-backend-unit`** або `pytest tests/backend/unit/` — запускає тільки unit тести бекенду
- **`make test-backend-integration`** або `pytest tests/backend/integration/` — запускає тільки інтеграційні тести бекенду
- **`make test-backend-e2e`** або `pytest tests/backend/e2e/` — запускає тільки e2e тести бекенду
- **`make test-frontend`** або `pytest tests/frontend/` — запускає тести фронтенду (наразі тільки placeholder)
- **`make test-ml`** або `pytest tests/ml/` — запускає всі ML тести (unit, experimental)
- **`make test-ml-unit`** або `pytest tests/ml/unit/` — запускає тільки unit тести ML
- **`make test-ml-experimental`** або `pytest tests/ml/experimental/` — запускає тільки експериментальні тести ML
- **`make test-experimental`** або `pytest tests/backend/experimental/ tests/ml/experimental/` — запускає всі експериментальні тести (бекенд + ML)

**Використання coverage** — команда `make test-coverage` або `pytest --cov=src --cov-report=html --cov-report=term tests/` запускає тести з вимірюванням покриття коду. Coverage вимірює відсоток коду, який виконується під час тестів, генерує HTML-звіт у `htmlcov/index.html` для детального перегляду покриття по файлах та рядках, та виводить текстовий звіт у термінал. Coverage допомагає виявити непокритий код, відстежити якість тестів та покращити покриття критичних частин системи.

**Конфігурація pytest** знаходиться у файлі `pytest.ini` з налаштуваннями:
- `testpaths = tests` — шляхи до тестів
- `python_files = test_*.py` — патерн для знаходження тестових файлів
- `python_classes = Test*` — патерн для тестових класів
- `python_functions = test_*` — патерн для тестових функцій
- Маркери для категорізації тестів (unit, integration, e2e, ml, experimental, slow, requires_models)
- `addopts = -v --strict-markers --tb=short --disable-warnings` — опції виводу (verbose, strict markers, short traceback, disable warnings)
- `asyncio_mode = auto` — автоматична обробка асинхронних тестів

## Покриття та якість

Покриття коду та якість тестів відстежуються через pytest-cov та аналіз структури тестів.

**Вимірювання coverage** — coverage вимірюється через `pytest-cov` при запуску тестів з опцією `--cov=src`. Coverage показує відсоток коду, який виконується під час тестів, детальний звіт по файлах, функціях та рядках, HTML-звіт для візуалізації покриття. Coverage допомагає виявити непокритий код, відстежити якість тестів та покращити покриття критичних частин системи.

**Частини системи, покриті тестами найкраще**:
- **Функції аутентифікації** — хешування паролів, JWT токени, перевірка паролів покриті unit тестами
- **Pydantic схеми** — валідація вхідних та вихідних даних покрита unit тестами
- **API ендпоінти** — автентифікація та прогнозування покриті інтеграційними тестами
- **Завантаження моделей** — завантаження чемпіонських та конкретних моделей покрито unit тестами
- **Логіка прогнозування** — діапазони ймовірностей, детермінізм покриті unit тестами

**Частини системи, покриті гірше або ще не покриті**:
- **Фронтенд** — наразі не покритий тестами (тільки placeholder)
- **Чати та повідомлення** — API для чатів може бути не покритий тестами
- **AI-асистент** — інтеграція з Ollama може бути не покрита тестами
- **Генерація PDF** — логіка генерації PDF на фронтенді не покрита тестами
- **Діаграми** — логіка побудови діаграм на фронтенді не покрита тестами
- **Системні статуси** — API-status може бути не покритий тестами
- **Блокування користувачів** — логіка блокування може бути не покрита тестами

**Як тести інтегруються з процесом розробки**:
- **Перед комітом** — розробники можуть запускати тести вручну через `make test` для перевірки що зміни не зламали існуючий функціонал
- **Перед релізом** — можна запускати всі тести з coverage для перевірки якості коду перед випуском нової версії
- **У CI** — наразі CI не налаштований, але в майбутньому можна додати автоматичний запуск тестів при кожному коміті, перевірку coverage, блокування мерджу якщо тести не проходять

## Можливі напрямки розвитку системи тестування

Система тестування має потенціал для розширення та покращення для забезпечення вищої якості коду та надійності системи.

**Додати e2e тести для повного сценарію користувача** — розширити e2e тести для покриття повних сценаріїв користувача від реєстрації до отримання PDF-звіту, включаючи навігацію між сторінками, заповнення форм, відображення результатів, генерацію звітів, роботу з чатами, використання AI-асистента.

**Покрити тестами усі критичні API** — додати тести для всіх API ендпоінтів, включаючи чати (`/api/chats/*`), AI-асистента (`/assistant/*`), системні статуси (`/health`, `/system/database/stats`), блокування користувачів (`/users/block`, `/users/unblock`), управління профілем (`/users/me`), історію прогнозів (`/users/history`).

**Додати стрес тести для ML моделей** — створити тести для перевірки продуктивності ML моделей при великій кількості одночасних запитів, великих обсягах даних, тривалих операціях, використанні пам'яті, часу відповіді.

**Додати тести PDF генерації** — створити тести для перевірки генерації PDF-звітів на фронтенді, включаючи коректність вмісту, форматування, вставки діаграм, підтримку кирилиці, розміри файлів, якість зображень.

**Додати більш глибокі тести для локалізації, доступності, перфомансу**:
- **Локалізація** — тести для перевірки коректності перекладів, відображення текстів українською та англійською, форматування дат/чисел, інтерполяції змінних
- **Доступність** — тести для перевірки ARIA-атрибутів, навігації з клавіатури, підтримки screen readers, контрастності кольорів
- **Перфоманс** — тести для перевірки часу завантаження сторінок, часу відповіді API, продуктивності рендерингу діаграм, оптимізації запитів до БД

**Додати тести для інтеграції з Ollama** — створити тести для перевірки інтеграції з локальним Ollama сервером, включаючи формування промптів, обробку відповідей, обробку помилок, таймаутів, збереження історії повідомлень.

**Додати тести для безпеки** — створити тести для перевірки безпеки системи, включаючи захист від SQL injection, XSS, CSRF, перевірку валідації вхідних даних, обмеження доступу до чужих даних, захист токенів.

**Додати тести для міграцій БД** — створити тести для перевірки міграцій схеми БД, включаючи додавання колонок, зміну типів, збереження даних при міграціях, відкат міграцій.

**Додати тести для генераторів тестових даних** — створити тести для перевірки коректності генераторів тестових даних у `tests/utils/test_data_generators.py`, включаючи реалістичність даних, екстремальні випадки, дані з пропущеними значеннями.

**Додати CI/CD інтеграцію** — налаштувати автоматичний запуск тестів при кожному коміті, перевірку coverage, блокування мерджу якщо тести не проходять, автоматичне тестування на різних версіях Python, автоматичне тестування на різних операційних системах.

