# ETL-процес: підготовка NHANES до health_dataset.csv

ETL (Extract, Transform, Load) — перший етап магістерської роботи, на якому сирі NHANES-таблиці об'єднуються в узгоджений придатний датасет. Саме на цьому етапі формується `health_dataset.csv`, який потім використовується в дослідницькому аналізі даних (EDA) та машинному навчанні (ML) для прогнозування ризиків здоров'я.

## Джерело сирих даних (NHANES)

NHANES (National Health and Nutrition Examination Survey) складається з десятків окремих таблиць, які містять різні категорії медичних та демографічних даних. Різні файли охоплюють:

- **Демографію** — вік, стать, раса/етнічність
- **Антропометрію** — зріст, вага, індекс маси тіла (ІМТ)
- **Рівні глюкози** — лабораторні вимірювання рівня глюкози в крові
- **Артеріальний тиск** — систолічний та діастолічний тиск
- **Холестерин** — загальний рівень холестерину
- **Інші медичні обстеження** — дані з опитувальників, інформація про прийом ліків, харчування

**Посилання на датасет:** https://www.kaggle.com/datasets/cdc/national-health-and-nutrition-examination-survey

Усі таблиці NHANES мають спільний унікальний ідентифікатор респондента — `SEQN` (Sequence Number), який дозволяє об'єднувати дані з різних джерел для кожного учасника дослідження.

## Файли та папки, що беруть участь у ETL

### Сирі дані

**Розташування:** `datasets/raw/`

У цій директорії зберігаються шість сирих CSV-файлів NHANES:

- `demographic.csv` — демографічні дані
- `examination.csv` — дані фізичного обстеження
- `labs.csv` — лабораторні показники
- `questionnaire.csv` — дані з опитувальників
- `diet.csv` — дані про харчування
- `medications.csv` — дані про прийом ліків

### ETL-логіка

**Основні скрипти:**

1. **`src/data/nhanes_etl.py`** — основний ETL-пайплайн з жорстко закодованими параметрами обробки
2. **`src/health_risk_ai/data/nhanes_etl.py`** — альтернативна реалізація ETL з використанням конфігураційного файлу

**Конфігурація:**

- **`configs/nhanes.yaml`** — YAML-файл з налаштуваннями ETL (шляхи до даних, список ознак, правила формування цільових змінних)

**CLI-інтерфейс:**

- **`scripts/cli.py`** — командний рядок для запуску ETL через команду `python -m scripts.cli data`

### Результат ETL

**Розташування:** `datasets/processed/health_dataset.csv`

Фінальний оброблений датасет зберігається у форматі CSV з кодуванням UTF-8, без індексів.

## Послідовність ETL-процесу

### 1. Завантаження сирих таблиць

ETL-процес починається з зчитування всіх шістьох сирих CSV-файлів з директорії `datasets/raw/`. Кожен файл завантажується з обробкою різних кодувань: спочатку намагається UTF-8, а у разі помилки використовується latin-1 як резервне кодування.

Ключова колонка для всіх таблиць — `SEQN` (Sequence Number), яка є унікальним ідентифікатором кожного респондента в дослідженні NHANES.

### 2. Об'єднання таблиць

Після завантаження всі таблиці об'єднуються в одну за допомогою зовнішнього злиття (outer join) за ключем `SEQN`. Це означає, що зберігаються всі респонденти, навіть якщо у них відсутні дані в деяких таблицях.

Процес об'єднання виконується послідовно: спочатку об'єднується перша таблиця з другою, потім результат з третьою, і так далі до об'єднання всіх шести таблиць. Це забезпечує повне зіставлення різних модулів NHANES для кожного респондента.

### 3. Вибір необхідних колонок

Після об'єднання всіх таблиць виконується відбір лише тих колонок, які необхідні для подальшого аналізу та моделювання. Логіка полягає в тому, що для EDA та ML потрібні тільки ключові ознаки, а не всі сотні колонок з оригінальних NHANES-таблиць.

Основні колонки, які зберігаються:

- **Демографія:** `SEQN`, `RIDAGEYR` (вік), `RIAGENDR` (стать)
- **Антропометрія:** `BMXBMI` (індекс маси тіла)
- **Артеріальний тиск:** `BPXSY1` (систолічний), `BPXDI1` (діастолічний)
- **Лабораторні показники:** `LBXGLU` (глюкоза), `LBXTC` (холестерин)
- **Статус захворювань:** `DIQ010` (діагностований діабет)

Якщо якась колонка відсутня в об'єднаній таблиці, вона просто пропускається, але процес продовжується з доступними колонками.

### 4. Обробка пропусків

ETL-процес вирішує проблему пропущених значень (missing values) наступним чином:

- **Видалення дублікатів:** спочатку видаляються дублікати за ключем `SEQN`, залишається лише перший запис для кожного унікального респондента
- **Фільтрація рядків з надмірними пропусками:** видаляються рядки, у яких більше 50% числових колонок містять пропущені значення. Це означає, що якщо у респондента відсутні дані більш ніж у половині ключових показників, такий запис виключається з датасету
- **Конвертація типів:** всі числові колонки конвертуються у тип float з обробкою помилок (некоректні значення перетворюються на NaN)

Окремі пропуски в колонках залишаються для подальшої обробки моделями машинного навчання або імпутації на наступних етапах.

### 5. Фільтрування

ETL-процес виконує базове фільтрування даних:

- **Видалення дублікатів:** забезпечується унікальність кожного респондента за ключем `SEQN`
- **Видалення неповних записів:** рядки з надмірною кількістю пропусків (понад 50%) видаляються
- **Валідація типів:** некоректні числові значення автоматично перетворюються на NaN під час конвертації типів

Конкретні перевірки на нереалістичні вимірювання (наприклад, дуже високий або від'ємний ІМТ) не виконуються на етапі ETL, але можуть бути додані під час EDA або перед навчанням моделей.

### 6. Перейменування колонок

У процесі ETL змінні зберігають свої оригінальні NHANES-назви (наприклад, `RIDAGEYR`, `BMXBMI`, `BPXSY1`). Явного перейменування колонок на більш читабельні назви не відбувається — це дозволяє легко звірятися з оригінальною документацією NHANES та забезпечує прозорість джерела даних.

### 7. Формування фінального датасету

На останньому етапі ETL створюються цільові змінні (target variables) на основі бізнес-правил:

- **`obesity_present`:** бінарна змінна, яка дорівнює 1, якщо `BMXBMI >= 30` (критерій ожиріння за WHO), інакше 0. Пропущені значення заповнюються як 0 (False)
- **`diabetes_present`:** бінарна змінна, яка дорівнює 1, якщо `DIQ010 == 1` (діагностований діабет згідно з опитувальником), інакше 0. Пропущені значення також заповнюються як 0

Після формування цільових змінних фінальний датасет зберігається у файл `datasets/processed/health_dataset.csv` у форматі CSV з кодуванням UTF-8, без індексів рядків.

Фінальний датасет містить:
- Усі респонденти з унікальним `SEQN`
- Ключові ознаки для моделювання (вік, стать, ІМТ, тиск, глюкоза, холестерин, статус діабету)
- Дві цільові змінні для бінарної класифікації (`obesity_present`, `diabetes_present`)

Цей файл використовується для:
- Дослідницького аналізу даних (EDA) — аналіз розподілів, кореляцій, візуалізацій
- Тренування моделей машинного навчання — навчання класифікаторів для прогнозування ризиків
- Інференсу через API — реальні прогнози для нових користувачів

## Перевірка якості підготовлених даних

Після завершення ETL-процесу виконуються контрольні перевірки якості даних:

### Перевірки на пропуски

Аналізується кількість та відсоток пропущених значень у кожній колонці. Ця інформація використовується для подальшого прийняття рішень щодо імпутації або видалення колонок.

### Перевірка типів даних

Всі числові колонки перевіряються на коректність типів (float для неперервних змінних, int для дискретних). Некоректні значення автоматично перетворюються на NaN під час конвертації.

### Перевірка діапазонів

Під час EDA виконується перевірка діапазонів значень (наприклад, ІМТ повинен бути в розумних межах, артеріальний тиск не може бути від'ємним). Ці перевірки допомагають виявити потенційні помилки в даних.

### Перевірка унікальності ключа SEQN

Перевіряється, що кожен `SEQN` зустрічається лише один раз у фінальному датасеті. Дублікати видаляються на етапі очищення даних.

Результати цих перевірок використовуються для наступного етапу EDA, де виконується детальний аналіз розподілів, кореляцій та візуалізацій. Звіти про якість даних зберігаються у директорії `artifacts/eda/` для подальшого використання.

## Як ETL інтегрується в загальну магістерську роботу

ETL є фундаментальним етапом магістерської роботи, на якому створюється єдиний узгоджений датасет для аналізу та моделювання. Правильне виконання ETL забезпечує якість моделей машинного навчання та достовірність прогнозів. Без цього етапу ML-моделі та API для реальних прогнозів були б неможливі, оскільки вони потребують структурованих, очищених та підготовлених даних у чітко визначеному форматі. ETL створює основу для всіх подальших етапів: дослідницького аналізу даних, тренування моделей та розробки веб-інтерфейсу для практичного застосування.

